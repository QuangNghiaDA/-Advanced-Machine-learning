{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Trần Quang Nghĩa - 19110392\n",
        "\n",
        "Lab 8 - MHNC"
      ],
      "metadata": {
        "id": "Di1JCSZZai64"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Câu 1: Giải thích vì sao khi dùng Naive-Learning thì xe taxi chỉ đứng yên một chỗ?**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IeQqrlqVaou-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vì quá trình khởi tạo có thể khiến xe rơi vào vị trí không thể di chuyển tiếp (ngõ cụt), lúc này xe vẫn giữ vị trí cũ, tuy nhiên reward vẫn được nhận cho action này. Điều này bắt nguồn từ việc random các action ban đầu khiến xe thực hiện sai (đón và trả khách) nên các giá trị reward rất nhỏ dẫn đến ở các bước sau, agent sẽ không chọn các action này thay vào đó là các action di chuyển."
      ],
      "metadata": {
        "id": "Lap4wu2yPVu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Câu 2: Giải thích vì sao khi dùng Q-Leaning thì xe taxi có thể đón và trả khách được?**"
      ],
      "metadata": {
        "id": "bgqatHZNasss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vì Q-Learning tìm ra các action tối ưu ở mỗi state. Do đó không có tình trạng chọn random như Naive-Learning. Ngoài ra, các cơ chế update Q-Value bằng các hệ số discount rate, learning rate làm cho các Q-Value luôn có sự chênh lệch nhất định, không bị trùng giá trị, dễ dàng tìm ra action tối ưu tại mỗi state."
      ],
      "metadata": {
        "id": "0VzuIpjTPZ1f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Câu 3: Tìm một game khác và thiết lập cho agent chơi được**"
      ],
      "metadata": {
        "id": "2zMTzEUwauxK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install dependencies"
      ],
      "metadata": {
        "id": "KjbhZcCwYrxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install python-opengl\n",
        "!apt install ffmpeg\n",
        "!apt install xvfb\n",
        "!pip3 install pyvirtualdisplay\n",
        "\n",
        "\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaWlvDhcUFl6",
        "outputId": "8b37341f-6a0c-4362-f25a-4cf813afb9ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  freeglut3\n",
            "Suggested packages:\n",
            "  libgle3\n",
            "The following NEW packages will be installed:\n",
            "  freeglut3 python-opengl\n",
            "0 upgraded, 2 newly installed, 0 to remove and 20 not upgraded.\n",
            "Need to get 570 kB of archives.\n",
            "After this operation, 5,733 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 freeglut3 amd64 2.8.1-3 [73.6 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n",
            "Fetched 570 kB in 2s (309 kB/s)\n",
            "Selecting previously unselected package freeglut3:amd64.\n",
            "(Reading database ... 124016 files and directories currently installed.)\n",
            "Preparing to unpack .../freeglut3_2.8.1-3_amd64.deb ...\n",
            "Unpacking freeglut3:amd64 (2.8.1-3) ...\n",
            "Selecting previously unselected package python-opengl.\n",
            "Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Setting up freeglut3:amd64 (2.8.1-3) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.11-0ubuntu0.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 20 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 20 not upgraded.\n",
            "Need to get 785 kB of archives.\n",
            "After this operation, 2,271 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.13 [785 kB]\n",
            "Fetched 785 kB in 1s (545 kB/s)\n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 126376 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.13_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.13) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.13) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(500, 500))\n",
        "virtual_display.start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QA8PurvmUKd-",
        "outputId": "25398457-2c80-40a8-f7ce-3583c4dbfd05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f3aa1c5ea30>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym\n",
        "!pip install git+https://github.com/ntasfi/PyGame-Learning-Environment.git\n",
        "!pip install git+https://github.com/qlan3/gym-games.git\n",
        "!pip install huggingface_hub\n",
        "\n",
        "!pip install pyyaml==6.0 # avoid key error metadata\n",
        "\n",
        "!pip install pyglet # Virtual Screen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wL1zH87UUYQu",
        "outputId": "76f7d6f3-0e32-406c-fce1-4720ee44030b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.8/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym) (1.21.6)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym) (5.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym) (3.11.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/ntasfi/PyGame-Learning-Environment.git\n",
            "  Cloning https://github.com/ntasfi/PyGame-Learning-Environment.git to /tmp/pip-req-build-w5v2hopb\n",
            "  Running command git clone -q https://github.com/ntasfi/PyGame-Learning-Environment.git /tmp/pip-req-build-w5v2hopb\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from ple==0.0.1) (1.21.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from ple==0.0.1) (7.1.2)\n",
            "Building wheels for collected packages: ple\n",
            "  Building wheel for ple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ple: filename=ple-0.0.1-py3-none-any.whl size=50789 sha256=f7271d0d2c80dbdaa8d7c1b1aa168602936a46447db09d74965cc1f2eec2da31\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-kvovaqyy/wheels/25/91/ce/6ff81989b50d0c0fbdea57938f1d3f27ff0f9087817925900c\n",
            "Successfully built ple\n",
            "Installing collected packages: ple\n",
            "Successfully installed ple-0.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/qlan3/gym-games.git\n",
            "  Cloning https://github.com/qlan3/gym-games.git to /tmp/pip-req-build-1hz8t65j\n",
            "  Running command git clone -q https://github.com/qlan3/gym-games.git /tmp/pip-req-build-1hz8t65j\n",
            "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.8/dist-packages (from gym-games==1.0.4) (1.21.6)\n",
            "Collecting MinAtar>=1.0.4\n",
            "  Downloading MinAtar-1.0.12-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: gym>=0.13.0 in /usr/local/lib/python3.8/dist-packages (from gym-games==1.0.4) (0.25.2)\n",
            "Requirement already satisfied: setuptools>=41.0.1 in /usr/local/lib/python3.8/dist-packages (from gym-games==1.0.4) (57.4.0)\n",
            "Collecting pygame>=1.9.6\n",
            "  Downloading pygame-2.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.8 MB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: ple>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from gym-games==1.0.4) (0.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym>=0.13.0->gym-games==1.0.4) (5.1.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym>=0.13.0->gym-games==1.0.4) (1.5.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym>=0.13.0->gym-games==1.0.4) (0.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym>=0.13.0->gym-games==1.0.4) (3.11.0)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.8/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (1.3.5)\n",
            "Requirement already satisfied: matplotlib>=3.0.3 in /usr/local/lib/python3.8/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (3.2.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.8/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (3.0.9)\n",
            "Requirement already satisfied: seaborn>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (0.11.2)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.8/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (1.7.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (0.11.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.8/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2018.9 in /usr/local/lib/python3.8/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (2022.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from ple>=0.0.1->gym-games==1.0.4) (7.1.2)\n",
            "Building wheels for collected packages: gym-games\n",
            "  Building wheel for gym-games (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym-games: filename=gym_games-1.0.4-py3-none-any.whl size=14631 sha256=a649c21fe2ea29c93d90bbca58c980bc9790cb7eaedc820ee1557498e395706c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-d1gbshho/wheels/7f/f5/b6/cf9b96eb6b799509d9f396fdc91fb01adc4492ad72ff34d38f\n",
            "Successfully built gym-games\n",
            "Installing collected packages: pygame, MinAtar, gym-games\n",
            "Successfully installed MinAtar-1.0.12 gym-games-1.0.4 pygame-2.1.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 29.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface_hub) (4.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from huggingface_hub) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface_hub) (6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface_hub) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface_hub) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface_hub) (3.8.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.9->huggingface_hub) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface_hub) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface_hub) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface_hub) (2022.12.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface_hub) (1.24.3)\n",
            "Installing collected packages: huggingface-hub\n",
            "Successfully installed huggingface-hub-0.11.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyyaml==6.0 in /usr/local/lib/python3.8/dist-packages (6.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyglet\n",
            "  Downloading pyglet-2.0.2.1-py3-none-any.whl (964 kB)\n",
            "\u001b[K     |████████████████████████████████| 964 kB 35.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyglet\n",
            "Successfully installed pyglet-2.0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Import the packages"
      ],
      "metadata": {
        "id": "Tr3qKsjPYz6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "import gym\n",
        "import gym_pygame\n",
        "\n",
        "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
        "\n",
        "import imageio"
      ],
      "metadata": {
        "id": "LKQQ8C3tUeAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "U99Z3YbWUhns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZtNtP8rUjVR",
        "outputId": "63fd8261-89b7-43e6-b97a-3226db9fc29d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Create the CartPole environment and understand how it works"
      ],
      "metadata": {
        "id": "bBQCEFHxZCQw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The CartPole-v1 environment\n",
        "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart.\n",
        "\n",
        "So, we start with CartPole-v1. The goal is to push the cart left or right so that the pole stays in the equilibrium.\n",
        "\n",
        "The episode ends if:\n",
        "\n",
        "- The pole Angle is greater than ±12°\n",
        "- Cart Position is greater than ±2.4\n",
        "- Episode length is greater than 500\n",
        "\n",
        "We get a reward of +1 every timestep the Pole stays in the equilibrium."
      ],
      "metadata": {
        "id": "A4rX8bpKZLQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_id = \"CartPole-v1\"\n",
        "# Create the env\n",
        "env = gym.make(env_id)\n",
        "\n",
        "# Create the evaluation env\n",
        "eval_env = gym.make(env_id)\n",
        "\n",
        "# Get the state space and action space\n",
        "s_size = env.observation_space.shape[0]\n",
        "a_size = env.action_space.n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWtrMkgrUpmz",
        "outputId": "7666143c-7654-4e84-9c76-621b87410305"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
        "print(\"The State Space is: \", s_size)\n",
        "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pM4dxkiKUsIr",
        "outputId": "cf90a990-39f9-4878-d774-024eb1262f59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_____OBSERVATION SPACE_____ \n",
            "\n",
            "The State Space is:  4\n",
            "Sample observation [-3.35756159e+00 -1.92392386e+38 -1.12792164e-01  2.50310754e+38]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
        "print(\"The Action Space is: \", a_size)\n",
        "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KP1pXtooUu3l",
        "outputId": "3b2650f9-752c-48af-e6e5-666497d3dc13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " _____ACTION SPACE_____ \n",
            "\n",
            "The Action Space is:  2\n",
            "Action Space Sample 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Build the Reinforce Architecture"
      ],
      "metadata": {
        "id": "uj9X_ZNeZw1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want:\n",
        "\n",
        "- Two fully connected layers (fc1 and fc2).\n",
        "- Using ReLU as activation function of fc1\n",
        "- Using Softmax to output a probability distribution over actions"
      ],
      "metadata": {
        "id": "9WqMDQFAZrmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self, s_size, a_size, h_size):\n",
        "        super(Policy, self).__init__()\n",
        "        self.fc1 = nn.Linear(s_size, h_size)\n",
        "        self.fc2 = nn.Linear(h_size, a_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.softmax(x, dim=1)\n",
        "    \n",
        "    def act(self, state):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        probs = self.forward(state).cpu()\n",
        "        m = Categorical(probs)\n",
        "        action = m.sample()\n",
        "        return action.item(), m.log_prob(action)"
      ],
      "metadata": {
        "id": "FpjoHsX9VlJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "debug_policy = Policy(s_size, a_size, 64).to(device)\n",
        "debug_policy.act(env.reset())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oefyvroIVo4I",
        "outputId": "b5562be9-be79-46f4-d648-4aaa8a3f3e26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, tensor([-0.7127], grad_fn=<SqueezeBackward1>))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Build the Reinforce Training Algorithm"
      ],
      "metadata": {
        "id": "u44RthmFZ6qP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n",
        "    # Help us to calculate the score during the training\n",
        "    scores_deque = deque(maxlen=100)\n",
        "    scores = []\n",
        "    # Line 3 of pseudocode\n",
        "    for i_episode in range(1, n_training_episodes+1):\n",
        "        saved_log_probs = []\n",
        "        rewards = []\n",
        "        state = env.reset()\n",
        "        # Line 4 of pseudocode\n",
        "        for t in range(max_t):\n",
        "            action, log_prob = policy.act(state)\n",
        "            saved_log_probs.append(log_prob)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            rewards.append(reward)\n",
        "            if done:\n",
        "                break \n",
        "        scores_deque.append(sum(rewards))\n",
        "        scores.append(sum(rewards))\n",
        "        \n",
        "        # Line 6 of pseudocode: calculate the return\n",
        "        ## Here, we calculate discounts for instance [0.99^1, 0.99^2, 0.99^3, ..., 0.99^len(rewards)]\n",
        "        discounts = [gamma**i for i in range(len(rewards)+1)]\n",
        "        ## We calculate the return by sum(gamma[t] * reward[t]) \n",
        "        R = sum([a*b for a,b in zip(discounts, rewards)])\n",
        "        \n",
        "        # Line 7:\n",
        "        policy_loss = []\n",
        "        for log_prob in saved_log_probs:\n",
        "            policy_loss.append(-log_prob * R)\n",
        "        policy_loss = torch.cat(policy_loss).sum()\n",
        "        \n",
        "        # Line 8:\n",
        "        optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if i_episode % print_every == 0:\n",
        "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
        "        \n",
        "    return scores"
      ],
      "metadata": {
        "id": "KPtO3jhLX3U9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train it\n",
        "\n",
        "- We're now ready to train our agent.\n",
        "- But first, we define a variable containing all the training hyperparameters."
      ],
      "metadata": {
        "id": "GFxF9M4LaGgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cartpole_hyperparameters = {\n",
        "    \"h_size\": 16,\n",
        "    \"n_training_episodes\": 1000,\n",
        "    \"n_evaluation_episodes\": 10,\n",
        "    \"max_t\": 1000,\n",
        "    \"gamma\": 1.0,\n",
        "    \"lr\": 1e-2,\n",
        "    \"env_id\": env_id,\n",
        "    \"state_space\": s_size,\n",
        "    \"action_space\": a_size,\n",
        "}"
      ],
      "metadata": {
        "id": "Q1UvWy-vYBXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create policy and place it to the device\n",
        "cartpole_policy = Policy(cartpole_hyperparameters[\"state_space\"], cartpole_hyperparameters[\"action_space\"], cartpole_hyperparameters[\"h_size\"]).to(device)\n",
        "cartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters[\"lr\"])"
      ],
      "metadata": {
        "id": "k8k5e5o1YEke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = reinforce(cartpole_policy,\n",
        "                   cartpole_optimizer,\n",
        "                   cartpole_hyperparameters[\"n_training_episodes\"], \n",
        "                   cartpole_hyperparameters[\"max_t\"],\n",
        "                   cartpole_hyperparameters[\"gamma\"], \n",
        "                   100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3YCKaS5YHww",
        "outputId": "97d3bcac-a129-4799-bf1d-295a6cc21a3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100\tAverage Score: 42.25\n",
            "Episode 200\tAverage Score: 57.94\n",
            "Episode 300\tAverage Score: 52.51\n",
            "Episode 400\tAverage Score: 47.84\n",
            "Episode 500\tAverage Score: 44.88\n",
            "Episode 600\tAverage Score: 60.09\n",
            "Episode 700\tAverage Score: 63.79\n",
            "Episode 800\tAverage Score: 53.42\n",
            "Episode 900\tAverage Score: 61.41\n",
            "Episode 1000\tAverage Score: 53.65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Define evaluation method "
      ],
      "metadata": {
        "id": "13Wz7G1QaS-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_agent(env, max_steps, n_eval_episodes, policy):\n",
        "  \"\"\"\n",
        "  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
        "  :param env: The evaluation environment\n",
        "  :param n_eval_episodes: Number of episode to evaluate the agent\n",
        "  :param policy: The Reinforce agent\n",
        "  \"\"\"\n",
        "  episode_rewards = []\n",
        "  for episode in range(n_eval_episodes):\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards_ep = 0\n",
        "    \n",
        "    for step in range(max_steps):\n",
        "      action, _ = policy.act(state)\n",
        "      new_state, reward, done, info = env.step(action)\n",
        "      total_rewards_ep += reward\n",
        "        \n",
        "      if done:\n",
        "        break\n",
        "      state = new_state\n",
        "    episode_rewards.append(total_rewards_ep)\n",
        "  mean_reward = np.mean(episode_rewards)\n",
        "  std_reward = np.std(episode_rewards)\n",
        "\n",
        "  return mean_reward, std_reward"
      ],
      "metadata": {
        "id": "4nae8UW_YciX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Evaluate our agent"
      ],
      "metadata": {
        "id": "FEiu4OfpaXcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_agent(eval_env, \n",
        "               cartpole_hyperparameters[\"max_t\"], \n",
        "               cartpole_hyperparameters[\"n_evaluation_episodes\"],\n",
        "               cartpole_policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6A6p8gjYfkR",
        "outputId": "dc3a45b2-6ba3-4942-9f39-e8f76c05c017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(41.6, 15.768322675541619)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    }
  ]
}